# ğŸ“š AI Study Partner

> **Context-Aware Question Answering from PDFs**

A production-ready RAG (Retrieval-Augmented Generation) system that intelligently answers questions from your PDF documents. Never worries about hallucinationsâ€”it only answers from what's in your uploaded content.

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ“„ **PDF Processing** | Extract and process text from multi-page PDFs |
| ğŸ” **Semantic Search** | Find relevant content using embeddings (not just keywords) |
| ğŸ’¡ **Context-Aware Answers** | Generate answers grounded in your documents |
| ğŸ›¡ï¸ **Anti-Hallucination** | Strict prompting ensures no made-up answers |
| ğŸ“‹ **Auto Summarization** | Generate summaries of your uploaded notes |
| ğŸ¨ **Web Interface** | Clean, user-friendly Gradio UI |

---

## ğŸ—ï¸ How It Works

This system implements a **Retrieval-Augmented Generation (RAG)** pipeline:

```
PDF â†’ Extraction â†’ Chunking â†’ Embeddings â†’ Vector Search â†’ Retrieval â†’ Answer Generation
```

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   User Question                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Convert to Embedding     â”‚
         â”‚  (semantic vector)        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Search Vector Store (FAISS)     â”‚
      â”‚  Find 5 most similar chunks      â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Retrieve Relevant PDF Chunks        â”‚
    â”‚  (with context around them)          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Generate Answer using T5 Model            â”‚
    â”‚  (with strict: "use only this context")    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Return Answer or "I don't know"       â”‚
    â”‚  (prevents hallucinations)             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

| Component | File | Purpose |
|-----------|------|---------|
| ğŸ“„ PDF Processor | `pdf_processor.py` | Extracts text from PDFs using PyPDF2 |
| âœ‚ï¸ Text Chunker | `text_chunker.py` | Splits text into semantic chunks with overlap |
| ğŸ”¢ Embeddings | `embeddings.py` | Converts text to semantic vectors |
| ğŸ“Š Vector Store | `vector_store.py` | FAISS-based similarity search |
| ğŸ¯ Retriever | `retriever.py` | Finds relevant chunks for queries |
| ğŸ’¬ Answer Generator | `answer_generator.py` | Generates grounded answers from context |
| ğŸ“ Summarizer | `summarizer.py` | Creates document summaries |
| ğŸ”— RAG Pipeline | `rag_pipeline.py` | Orchestrates all components |
| ğŸ¨ UI | `app.py` | Gradio web interface |

---

## âš¡ Quick Start

### Prerequisites
- Python 3.8+ 
- pip package manager

### Installation (3 Steps)

**1. Install dependencies:**
```bash
pip install -r requirements.txt
```

> **Note:** First run downloads pre-trained models (~2-3 GB):
> - `all-MiniLM-L6-v2` (embeddings, ~90 MB)
> - `google/flan-t5-base` (answering, ~990 MB)  
> - `facebook/bart-large-cnn` (summarization, ~1.6 GB)

**2. Launch the app:**
```bash
python app.py
```

**3. Open in browser:**
```
http://localhost:7860
```

### Your First Question

1. ğŸ“¤ Upload a PDF (lecture notes, textbook, etc.)
2. â³ Click "Process PDF" and wait for indexing
3. â“ Switch to "Ask Questions" tab
4. ğŸ’¬ Try: *"What is this document about?"*
5. âœ… Get an answer grounded in your PDF!

### Programmatic Usage

See [example_usage.py](example_usage.py) for direct Python integration:

```python
from rag_pipeline import RAGPipeline

pipeline = RAGPipeline()
chunks, text = pipeline.process_pdf("notes.pdf")
pipeline.index_documents(chunks)
answer, context_chunks = pipeline.answer_question("What's the main topic?")
print(f"Answer: {answer}")
```

---

## ğŸ“ How It Works (Detailed)

### 1ï¸âƒ£ Text Chunking: Breaking Documents Into Pieces

**Why chunk?** Large documents exceed model limits. Chunking allows processing long PDFs efficiently.

```
Original: "Machine learning is... [500 words] ...deep learning."

After Chunking:
â”œâ”€ Chunk 1: "Machine learning is... [500 tokens]"
â”œâ”€ Chunk 2: "[100 token overlap] ...deep learning [400 new]"
â””â”€ Chunk 3: "[100 token overlap] ...continues..."
```

**Our Strategy:**
- **Size:** ~500 tokens per chunk (optimal balance)
- **Overlap:** ~100 tokens (preserves context at boundaries)
- **Sentence-Aware:** Never cuts mid-sentence (better semantics)
- **Benchmark:** 1 token â‰ˆ 0.75 words in English

---

### 2ï¸âƒ£ Embeddings: Turning Words Into Numbers

**Concept:** Convert text into numerical vectors that capture semantic meaning.

```
Text                    â†’  Embedding Vector (384 dimensions)
"car"                   â†’  [0.123, -0.456, 0.789, ...]
"automobile"            â†’  [0.124, -0.455, 0.791, ...]  â† Very similar!
"weather is sunny"      â†’  [-0.800, 0.234, -0.567, ...]  â† Different!
```

**Keyword Search vs. Semantic Search:**

| Aspect | Keyword | Semantic |
|--------|---------|----------|
| **Synonyms** | âŒ "car" â‰  "automobile" | âœ… Handles synonyms |
| **Context** | âŒ "bank" confused (river/finance) | âœ… Understands context |
| **Flexibility** | âŒ Exact match only | âœ… Similar meanings work |
| **Quality** | âŒ High false positives | âœ… Accurate results |

**Our Model: `all-MiniLM-L6-v2`**
- 384-dimensional embeddings
- Fast & efficient
- High-quality semantic search
- Optimized for cosine similarity

---

### 3ï¸âƒ£ Vector Search: Finding Relevant Chunks

**Process:**
```
User Question: "What is neural network?"
    â†“
Convert to Embedding [0.456, -0.123, ...]
    â†“
Compare with 5,000+ document chunks
    â†“
Return Top 5 most similar chunks
    â†“
Display to answer generator
```

**Technology: FAISS (Facebook AI Similarity Search)**
- Ultra-fast similarity search
- Uses L2 distance (Euclidean) for normalized embeddings
- Handles millions of vectors efficiently
- Production-ready performance

---

### 4ï¸âƒ£ RAG: Retrieval + Generation

**What's RAG?** Combines retrieval (finding info) + generation (creating answers)

```
Traditional LLM Problems:
- âŒ Makes up facts (hallucination)
- âŒ Uses only training data knowledge
- âŒ Can't learn from new documents

RAG Solution:
- âœ… Answers only from YOUR documents
- âœ… Transparent: shows source chunks
- âœ… "I don't know" when answer missing
```

**Our RAG Workflow:**
1. User asks question
2. System retrieves relevant PDF chunks
3. System generates answer from retrieved context
4. If no relevant chunks â†’ "I don't know"

---

### 5ï¸âƒ£ Answer Generation: T5 Model

**Model: `google/flan-t5-base`**
- T5 (Text-to-Text Transfer Transformer) architecture
- 250M parameters (efficient & fast)
- Trained on diverse NLP tasks
- Excellent for QA without hallucination

**Anti-Hallucination Prompt:**
```
Answer the question using ONLY the information provided below. 
If the context does not contain the answer, respond with "I don't know."

Context: [Your PDF chunks here]
Question: [User question]
Answer (using ONLY context above):
```

---

### 6ï¸âƒ£ Summarization: BART Model

**Model: `facebook/bart-large-cnn`**
- Bidirectional Auto-Regressive Transformer (BART)
- Trained on CNN/DailyMail news (excellent summarization)
- Generates concise overviews
- Great for understanding document structure

**Use Cases:**
- Quick overview of lecture notes
- Understand what's in a document before asking questions
- Create study guides

---

## âš™ï¸ Configuration

Customize the pipeline in [app.py](app.py):

```python
pipeline = RAGPipeline(
    embedding_model="all-MiniLM-L6-v2",         # Semantic embeddings
    answer_model="google/flan-t5-base",         # QA generation  
    summarizer_model="facebook/bart-large-cnn", # Summarization
    chunk_size=500,                             # Tokens per chunk
    chunk_overlap=100,                          # Overlap tokens
    top_k=5                                     # Retrieved chunks
)
```

---

## âš ï¸ Limitations & Considerations

| Limitation | Details | Workaround |
|-----------|---------|-----------|
| **Model Size** | T5-base is small (250M params), struggles with complex reasoning | Use T5-large or larger models |
| **PDF Type** | Works with text PDFs; scanned images need OCR | Use high-quality text PDFs |
| **Complex Layouts** | Tables, multi-column text may extract poorly | Pre-process PDFs if needed |
| **Token Limits** | Max 512 tokens for T5 model | Increase chunk size or use longer-context models |
| **Retrieval Gaps** | May miss chunks if query wording differs significantly | Rephrase questions differently |
| **Single PDF** | Processes one PDF at a time | Can extend to multi-PDF support |
| **No Memory** | Each question is independent | Can add conversation history |
| **Long Answers** | Generates concise answers, not long-form content | Fine-tune model for longer outputs |

---

## ğŸ’¡ Usage Examples

### Example 1: Basic Q&A

```
ğŸ“¤ Upload: lecture_notes.pdf
âœ… Process PDF
â“ Question: "What are the main topics covered?"
ğŸ’¬ Answer: [Generated from your PDF]
```

### Example 2: Get a Summary

```
ğŸ“¤ Upload: textbook_chapter.pdf
ğŸ”„ Go to "Summarize" tab
ğŸ“ Click "Generate Summary"
âœ… Get concise overview of content
```

### Example 3: Programmatic Integration

```python
from rag_pipeline import RAGPipeline

# Initialize
pipeline = RAGPipeline()

# Process PDF
chunks, text = pipeline.process_pdf("notes.pdf")
pipeline.index_documents(chunks)

# Ask multiple questions
questions = [
    "What is the main topic?",
    "List the key concepts",
    "Summarize in one sentence"
]

for q in questions:
    answer, sources = pipeline.answer_question(q)
    print(f"Q: {q}\nA: {answer}\n")
```

---

## ğŸ”¬ Technical Details

### Token Counting
```
"Hello world, this is a test."
â†“
T5 Tokenizer
â†“
['Hello', 'world', ',', 'this', 'is', 'a', 'test', '.']
â†“
8 tokens â‰ˆ 10-11 words
```
**Rule of thumb:** 1 token â‰ˆ 0.75 words in English

### Similarity Scoring
- **Range:** 0.0 (completely different) to 1.0 (identical)
- **Default threshold:** 0.3 (30% match)
- **Lower threshold** = more results (may include noise)
- **Higher threshold** = fewer results (may miss relevant)

### Retrieval Strategy
```
Retrieved Chunks = 5 (default)
           â†“
More chunks = more context (slower)
Fewer chunks = faster (may miss context)
```

---

## ğŸ“‚ Project Structure

```
StudyBuddy/
â”‚
â”œâ”€â”€ ğŸ¨ UI & Main
â”‚   â”œâ”€â”€ app.py                 # Gradio web interface
â”‚   â””â”€â”€ example_usage.py       # Python integration example
â”‚
â”œâ”€â”€ ğŸ”— Core Pipeline
â”‚   â””â”€â”€ rag_pipeline.py        # Main orchestrator
â”‚
â”œâ”€â”€ ğŸ“„ Document Processing
â”‚   â”œâ”€â”€ pdf_processor.py       # Extract text from PDFs
â”‚   â””â”€â”€ text_chunker.py        # Smart text chunking
â”‚
â”œâ”€â”€ ğŸ§  AI Models
â”‚   â”œâ”€â”€ embeddings.py          # Generate embeddings
â”‚   â”œâ”€â”€ answer_generator.py    # Generate answers (T5)
â”‚   â””â”€â”€ summarizer.py          # Summarize docs (BART)
â”‚
â”œâ”€â”€ ğŸ“Š Data Management
â”‚   â”œâ”€â”€ vector_store.py        # FAISS vector database
â”‚   â”œâ”€â”€ retriever.py           # Semantic retrieval
â”‚   â””â”€â”€ requirements.txt       # Python dependencies
â”‚
â””â”€â”€ ğŸ“– Documentation
    â””â”€â”€ README.md              # This file
```

---

## ğŸ› ï¸ How to Extend

### Add Multi-PDF Support
```python
# Support multiple PDFs in one search
pipeline.add_pdf("notes.pdf")
pipeline.add_pdf("textbook.pdf")
answer = pipeline.answer_question("Combine knowledge from both")
```

### Improve Answer Quality
- Use larger models: `google/flan-t5-large` or `gpt-3.5`
- Implement chunk re-ranking
- Add confidence scores
- Fine-tune on domain data

### Add Conversation Memory
- Store conversation history
- Use previous context in retrieval
- Implement follow-up questions
- Add chat-like interactions

### Support More File Types
```python
# Easy to add:
- .docx files (python-docx)
- .txt files (plain text)
- .md files (markdown)
- Web pages (requests + BeautifulSoup)
```

---

## ğŸ“Š Performance Metrics

| Component | Speed | Model Size | Memory |
|-----------|-------|------------|--------|
| **Embeddings** | ~1ms per chunk | 90 MB | Low |
| **Retrieval** | ~5-10ms (5 chunks) | In-memory | Variable |
| **Answer Gen** | ~1-2s per question | 990 MB | ~2 GB |
| **Summarization** | ~3-5s per doc | 1.6 GB | ~2 GB |

*Measured on CPU; GPU would be 5-10x faster*

---

## ğŸ“ Troubleshooting

**Q: Model downloads are slow?**  
A: First-time setup downloads ~2-3 GB. Be patient! Subsequent runs use cache.

**Q: "I don't know" for every question?**  
A: Check if PDF processed correctly, try different question phrasing, or upload higher-quality PDF.

**Q: Answers seem off-topic?**  
A: Your PDF might have poor text extraction. Try different PDF or rephrase question.

**Q: Running out of memory?**  
A: Reduce `chunk_size`, process smaller PDFs, or use GPU acceleration.

---

## ğŸ“š Learn More

- **RAG Papers:** [Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401)
- **T5 Model:** [Exploring the Limits of Transfer Learning with T5](https://arxiv.org/abs/1910.10683)
- **FAISS:** [Billion-scale Similarity Search](https://ai.facebook.com/blog/faiss-a-library-for-efficient-similarity-search/)
- **HuggingFace:** [Transformers Models Hub](https://huggingface.co/models)

---

## ğŸ“„ License

This project is provided as-is for educational and research purposes.

---

## ğŸ™ Acknowledgments

Built with:
- ğŸ¤— **HuggingFace** - Transformer models and tokenizers
- ğŸ” **FAISS** - Facebook AI's similarity search library
- ğŸ¨ **Gradio** - Simple web interfaces for ML models
- ğŸ”¥ **PyTorch** - Deep learning framework
- ğŸ“„ **PyPDF2** - PDF text extraction

---

## ğŸ¯ Next Steps

- â­ Star this repo if you find it useful!
- ğŸ“ Try it with your own PDFs
- ğŸ”§ Customize models and parameters
- ğŸš€ Extend with new features
- ğŸ’¬ Share feedback and improvements

**Happy studying! ğŸ“šâœ¨**

---

<p align="center">
  <b>Built with â¤ï¸ using PyTorch, HuggingFace Transformers, and FAISS</b>
</p>
